{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199906ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights, ResNet\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3eece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class_names = ['Aster', 'Daisy', 'Iris', 'Lavender', 'Lily', 'Marigold', 'Orchid', 'Poppy', 'Rose', 'Sunflower']\n",
    "\n",
    "torch.serialization.add_safe_globals([ResNet])  # ou torchvision.models.resnet.ResNet\n",
    "\n",
    "model = torch.load(\n",
    "    './Models/resnet50_flowers_dataset-10-epochs.h5',\n",
    "    weights_only=False  # garante que ele vai carregar o modelo completo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6468411",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2944683",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e1a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\vhlab\\\\Desktop\\\\Trabalho ML\\\\https:\\\\images.tcdn.com.br\\\\img\\\\img_prod\\\\828138\\\\bouquet_com_50_rosas_vermelhas_explorer_801_1_08b8cd329c897dabe4d56706b67d1dc4.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m img_path = \u001b[33m'\u001b[39m\u001b[33mhttps://images.tcdn.com.br/img/img_prod/828138/bouquet_com_50_rosas_vermelhas_explorer_801_1_08b8cd329c897dabe4d56706b67d1dc4.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m input_tensor = transform(image).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vhlab\\Desktop\\Trabalho ML\\.venv\\Lib\\site-packages\\PIL\\Image.py:3469\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3466\u001b[39m     filename = os.path.realpath(os.fspath(fp))\n\u001b[32m   3468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[32m-> \u001b[39m\u001b[32m3469\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3470\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\vhlab\\\\Desktop\\\\Trabalho ML\\\\https:\\\\images.tcdn.com.br\\\\img\\\\img_prod\\\\828138\\\\bouquet_com_50_rosas_vermelhas_explorer_801_1_08b8cd329c897dabe4d56706b67d1dc4.jpg'"
     ]
    }
   ],
   "source": [
    "url = 'https://images.tcdn.com.br/img/img_prod/828138/bouquet_com_50_rosas_vermelhas_explorer_801_1_08b8cd329c897dabe4d56706b67d1dc4.jpg'\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    _, predicted = torch.max(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01301e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = predicted.item()\n",
    "class_name = class_names[class_idx]\n",
    "print(f'Classe prevista: {class_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
